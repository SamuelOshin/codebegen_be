#!/usr/bin/env python3
"""
Offline AI Model Infrastructure Test
Tests model loading framework without requiring internet connectivity
"""

import asyncio
import sys
import os
import torch
from pathlib import Path


class OfflineModelTester:
    """Test model infrastructure without downloading models"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üîß Device: {self.device}")
        
    def test_pytorch_installation(self):
        """Test PyTorch installation and capabilities"""
        print("üß™ Testing PyTorch Installation...")
        
        try:
            print(f"   ‚úÖ PyTorch Version: {torch.__version__}")
            print(f"   ‚úÖ CUDA Available: {torch.cuda.is_available()}")
            if torch.cuda.is_available():
                print(f"   ‚úÖ CUDA Version: {torch.version.cuda}")
                print(f"   ‚úÖ GPU Count: {torch.cuda.device_count()}")
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    print(f"   ‚úÖ GPU {i}: {props.name} ({props.total_memory / 1024**3:.1f}GB)")
            
            # Test basic tensor operations
            x = torch.randn(3, 3)
            y = torch.randn(3, 3)
            z = x @ y
            print(f"   ‚úÖ Matrix multiplication: Working (shape: {z.shape})")
            
            # Test device operations
            if torch.cuda.is_available():
                x_gpu = x.to("cuda")
                y_gpu = y.to("cuda")
                z_gpu = x_gpu @ y_gpu
                print(f"   ‚úÖ GPU operations: Working")
                
                # Memory test
                allocated = torch.cuda.memory_allocated() / 1024**2
                print(f"   ‚úÖ GPU memory allocated: {allocated:.2f}MB")
            
            # Test autograd
            x.requires_grad_(True)
            y.requires_grad_(True)
            z = (x @ y).sum()
            z.backward()
            print(f"   ‚úÖ Autograd: Working (x.grad shape: {x.grad.shape})")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå PyTorch test failed: {e}")
            return False
    
    def test_transformers_import(self):
        """Test transformers library import and basic functionality"""
        print("\nüß™ Testing Transformers Library...")
        
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
            print("   ‚úÖ Transformers imported successfully")
            
            # Test tokenizer creation (offline)
            try:
                from transformers import GPT2TokenizerFast
                tokenizer = GPT2TokenizerFast(
                    vocab_file=None,
                    merges_file=None,
                    tokenizer_file=None,
                    unk_token="<|endoftext|>",
                    bos_token="<|endoftext|>",
                    eos_token="<|endoftext|>",
                    pad_token="<|endoftext|>",
                )
                print("   ‚úÖ Tokenizer creation: Working")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Tokenizer test skipped: {e}")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå Transformers test failed: {e}")
            return False
    
    def test_model_loader_framework(self):
        """Test our model loader framework"""
        print("\nüß™ Testing Model Loader Framework...")
        
        try:
            from ai_models.model_loader import ModelLoader, ModelType, ModelInfo
            
            # Test ModelLoader initialization
            loader = ModelLoader()
            print(f"   ‚úÖ ModelLoader initialized")
            print(f"   ‚úÖ Max concurrent models: {loader.max_concurrent_models}")
            print(f"   ‚úÖ Available model types: {[t.value for t in ModelType]}")
            
            # Test model registration
            from ai_models.qwen_generator import QwenGenerator
            print("   ‚úÖ QwenGenerator imported successfully")
            
            # Test model info structure
            fake_model_info = ModelInfo(
                model_type=ModelType.QWEN_GENERATOR,
                model_path="/fake/path",
                loaded=False
            )
            print(f"   ‚úÖ ModelInfo structure: {fake_model_info}")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå Model loader test failed: {e}")
            return False
    
    def test_individual_generators(self):
        """Test individual generator imports"""
        print("\nüß™ Testing Individual Generators...")
        
        results = {}
        generators = [
            ("QwenGenerator", "ai_models.qwen_generator"),
            ("LlamaParser", "ai_models.llama_parser"),
            ("StarcoderReviewer", "ai_models.starcoder_reviewer"),
            ("MistralDocsGenerator", "ai_models.mistral_docs"),
        ]
        
        for name, module_path in generators:
            try:
                module = __import__(module_path, fromlist=[name])
                generator_class = getattr(module, name)
                print(f"   ‚úÖ {name}: Imported successfully")
                results[name] = True
            except Exception as e:
                print(f"   ‚ùå {name}: Failed - {e}")
                results[name] = False
        
        return all(results.values())
    
    def test_memory_management(self):
        """Test memory management capabilities"""
        print("\nüß™ Testing Memory Management...")
        
        try:
            import gc
            import psutil
            import os
            
            # Get current memory usage
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss / 1024**2  # MB
            print(f"   ‚úÖ Initial memory usage: {initial_memory:.2f}MB")
            
            # Create some tensors
            tensors = []
            for i in range(10):
                tensor = torch.randn(1000, 1000)
                if torch.cuda.is_available():
                    tensor = tensor.to("cuda")
                tensors.append(tensor)
            
            current_memory = process.memory_info().rss / 1024**2
            print(f"   ‚úÖ Memory after tensor creation: {current_memory:.2f}MB")
            print(f"   ‚úÖ Memory increase: {current_memory - initial_memory:.2f}MB")
            
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024**2
                print(f"   ‚úÖ GPU memory allocated: {gpu_memory:.2f}MB")
            
            # Clean up
            del tensors
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            final_memory = process.memory_info().rss / 1024**2
            print(f"   ‚úÖ Memory after cleanup: {final_memory:.2f}MB")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå Memory management test failed: {e}")
            return False
    
    async def test_async_framework(self):
        """Test async framework compatibility"""
        print("\nüß™ Testing Async Framework...")
        
        try:
            async def mock_model_load():
                """Mock async model loading"""
                await asyncio.sleep(0.1)
                return "Model loaded"
            
            async def test_concurrent_loads():
                """Test concurrent model loading"""
                tasks = [mock_model_load() for _ in range(3)]
                results = await asyncio.gather(*tasks)
                return results
            
            # Test concurrent loading
            results = await test_concurrent_loads()
            print(f"   ‚úÖ Concurrent loading test: {len(results)} operations completed")
            
            # Test single async operation
            result = await mock_model_load()
            print(f"   ‚úÖ Async operation: {result}")
            
            # Test thread pool executor pattern (used by our model loader)
            loop = asyncio.get_event_loop()
            
            def sync_operation():
                return "Sync operation completed"
            
            result = await loop.run_in_executor(None, sync_operation)
            print(f"   ‚úÖ Thread pool executor: {result}")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå Async framework test failed: {e}")
            return False


async def main():
    """Run comprehensive offline model infrastructure tests"""
    print("üéØ Offline AI Model Infrastructure Test")
    print("=" * 60)
    print("üìã Testing model loading framework without internet connectivity")
    print("=" * 60)
    
    tester = OfflineModelTester()
    results = {}
    
    # Test 1: PyTorch Installation
    results['pytorch'] = tester.test_pytorch_installation()
    
    # Test 2: Transformers Import
    results['transformers'] = tester.test_transformers_import()
    
    # Test 3: Model Loader Framework
    results['model_loader'] = tester.test_model_loader_framework()
    
    # Test 4: Individual Generators
    results['generators'] = tester.test_individual_generators()
    
    # Test 5: Memory Management
    results['memory'] = tester.test_memory_management()
    
    # Test 6: Async Framework
    results['async'] = await tester.test_async_framework()
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä OFFLINE TEST RESULTS SUMMARY")
    print("=" * 60)
    
    for test_name, success in results.items():
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{test_name.capitalize():15} : {status}")
    
    total_passed = sum(results.values())
    total_tests = len(results)
    
    print(f"\nPassed: {total_passed}/{total_tests}")
    
    if total_passed == total_tests:
        print("\nüéâ All offline tests passed! Model infrastructure is ready!")
        print("\nüìã Validated Infrastructure:")
        print("   ‚úÖ PyTorch 2.8.0 working with tensor operations")
        print("   ‚úÖ Transformers library functional") 
        print("   ‚úÖ Model loader framework operational")
        print("   ‚úÖ Generator classes importable")
        print("   ‚úÖ Memory management working")
        print("   ‚úÖ Async framework compatible")
        print("\nüåê Next Step: Test with internet connectivity to download models")
    else:
        print(f"\n‚ö†Ô∏è  {total_tests - total_passed} tests failed. Fix infrastructure issues first.")
    
    return total_passed == total_tests


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
